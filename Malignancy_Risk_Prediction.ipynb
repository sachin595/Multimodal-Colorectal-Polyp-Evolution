{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4tStxyN-Xwd",
        "outputId": "e2c6f0a6-86c5-4dc5-beda-0e4db40ef1d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Paths to the files in Google Drive\n",
        "train_data_path = '/content/drive/My Drive/Train_data.csv'\n",
        "test_data_path = '/content/drive/My Drive/Test_data.csv'\n",
        "val_data_path = '/content/drive/My Drive/Val_data.csv'\n",
        "\n",
        "# Load the CSV files\n",
        "train_data = pd.read_csv(train_data_path)\n",
        "test_data = pd.read_csv(test_data_path)\n",
        "val_data = pd.read_csv(val_data_path)\n",
        "\n",
        "# Combine the datasets\n",
        "combined_data = pd.concat([train_data, test_data, val_data], axis=0, ignore_index=True)\n",
        "\n",
        "# Check the combined data\n",
        "print(f\"Combined dataset contains {combined_data.shape[0]} rows and {combined_data.shape[1]} columns.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nBgjCPXh-gMi",
        "outputId": "91d29268-c339-4053-b24f-144b3cbd2210"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset contains 4727 rows and 83 columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping 'File_Path' and 'Split' columns from the dataset\n",
        "combined_data = combined_data.drop(columns=['File_Path', 'Split'])\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(combined_data.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-tR0hVkMCJo",
        "outputId": "7b9bcc0c-0013-4260-ec11-d3e1bb8d2e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Patient_ID', 'Age', 'Size_cm', 'Recurrence', 'Sex_F', 'Sex_M',\n",
            "       'Polyp_Location_ Descending', 'Polyp_Location_ Sigmoid',\n",
            "       'Polyp_Location_ Splenic Flexure', 'Polyp_Location_Anastomosis',\n",
            "       'Polyp_Location_Asc / Transverse ?', 'Polyp_Location_Ascending',\n",
            "       'Polyp_Location_Cecum', 'Polyp_Location_Cecum\\nKissing Polyps',\n",
            "       'Polyp_Location_Descending', 'Polyp_Location_Descending ',\n",
            "       'Polyp_Location_Hepatic Flexure', 'Polyp_Location_Ileocecal Valve',\n",
            "       'Polyp_Location_Ileocecal Valve\\nKissing Polyps',\n",
            "       'Polyp_Location_Rectosigmoid', 'Polyp_Location_Rectosigmoid ',\n",
            "       'Polyp_Location_Rectum', 'Polyp_Location_Sigmoid',\n",
            "       'Polyp_Location_Sigmoid, Splenic Flexure',\n",
            "       'Polyp_Location_Splenic Flexure', 'Polyp_Location_Transvers Colon',\n",
            "       'Polyp_Location_Transverse', 'Circum_0.3333333333333333', 'Circum_<1/3',\n",
            "       'Circum_>1/3', 'Cross_Two_Folds_Between Folds', 'Cross_Two_Folds_Neg',\n",
            "       'Cross_Two_Folds_On Fold', 'Cross_Two_Folds_On Fold ',\n",
            "       'Cross_Two_Folds_Pos', 'Paris_0-IIa', 'Paris_0-IIa + IIc',\n",
            "       'Paris_0-IIa + Is', 'Paris_0-IIa /c', 'Paris_0-IIb', 'Paris_0-Ip',\n",
            "       'Paris_0-Ips', 'Paris_0-Is', 'Paris_0-lps', 'Paris_0-ls',\n",
            "       'LST_Type_LST DT', 'LST_Type_LST-G HT', 'LST_Type_LST-G MN',\n",
            "       'LST_Type_LST-NG FT', 'LST_Type_LST-NG PD', 'Pit_2A', 'Pit_2B', 'Pit_I',\n",
            "       'Pit_II', 'Pit_III', 'Pit_III, IV', 'Pit_III, IV, V', 'Pit_III, V',\n",
            "       'Pit_IIIs', 'Pit_IV', 'Pit_IV, V', 'Pit_V', 'JNET_1', 'JNET_2A',\n",
            "       'JNET_2B', 'JNET_3', 'Diagnosis_Adenocarcinoma',\n",
            "       'Diagnosis_Hyperplastic', 'Diagnosis_Inflammatory',\n",
            "       'Diagnosis_Serrated', 'Diagnosis_Serrated, Hyperplastic',\n",
            "       'Diagnosis_T + V', 'Diagnosis_Traditional Serrated Adenoma',\n",
            "       'Diagnosis_Tubular', 'Diagnosis_Villous',\n",
            "       'Dysplasia_Grade_Differentiation_HGD',\n",
            "       'Dysplasia_Grade_Differentiation_LGD',\n",
            "       'Dysplasia_Grade_Differentiation_Mod Diff\\nOrigin: Tubulovillous',\n",
            "       'Dysplasia_Grade_Differentiation_Mod: Well Diff \\nOrigin: Tubulovillous',\n",
            "       'Dysplasia_Grade_Differentiation_T + V',\n",
            "       'Dysplasia_Grade_Differentiation_Well Diff'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "combined_data.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqjJLmgW_VWe",
        "outputId": "957513b0-4f3d-4e78-8750-c9e40ad735a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Patient_ID', 'Age', 'Size_cm', 'Recurrence', 'Sex_F', 'Sex_M',\n",
              "       'Polyp_Location_ Descending', 'Polyp_Location_ Sigmoid',\n",
              "       'Polyp_Location_ Splenic Flexure', 'Polyp_Location_Anastomosis',\n",
              "       'Polyp_Location_Asc / Transverse ?', 'Polyp_Location_Ascending',\n",
              "       'Polyp_Location_Cecum', 'Polyp_Location_Cecum\\nKissing Polyps',\n",
              "       'Polyp_Location_Descending', 'Polyp_Location_Descending ',\n",
              "       'Polyp_Location_Hepatic Flexure', 'Polyp_Location_Ileocecal Valve',\n",
              "       'Polyp_Location_Ileocecal Valve\\nKissing Polyps',\n",
              "       'Polyp_Location_Rectosigmoid', 'Polyp_Location_Rectosigmoid ',\n",
              "       'Polyp_Location_Rectum', 'Polyp_Location_Sigmoid',\n",
              "       'Polyp_Location_Sigmoid, Splenic Flexure',\n",
              "       'Polyp_Location_Splenic Flexure', 'Polyp_Location_Transvers Colon',\n",
              "       'Polyp_Location_Transverse', 'Circum_0.3333333333333333', 'Circum_<1/3',\n",
              "       'Circum_>1/3', 'Cross_Two_Folds_Between Folds', 'Cross_Two_Folds_Neg',\n",
              "       'Cross_Two_Folds_On Fold', 'Cross_Two_Folds_On Fold ',\n",
              "       'Cross_Two_Folds_Pos', 'Paris_0-IIa', 'Paris_0-IIa + IIc',\n",
              "       'Paris_0-IIa + Is', 'Paris_0-IIa /c', 'Paris_0-IIb', 'Paris_0-Ip',\n",
              "       'Paris_0-Ips', 'Paris_0-Is', 'Paris_0-lps', 'Paris_0-ls',\n",
              "       'LST_Type_LST DT', 'LST_Type_LST-G HT', 'LST_Type_LST-G MN',\n",
              "       'LST_Type_LST-NG FT', 'LST_Type_LST-NG PD', 'Pit_2A', 'Pit_2B', 'Pit_I',\n",
              "       'Pit_II', 'Pit_III', 'Pit_III, IV', 'Pit_III, IV, V', 'Pit_III, V',\n",
              "       'Pit_IIIs', 'Pit_IV', 'Pit_IV, V', 'Pit_V', 'JNET_1', 'JNET_2A',\n",
              "       'JNET_2B', 'JNET_3', 'Diagnosis_Adenocarcinoma',\n",
              "       'Diagnosis_Hyperplastic', 'Diagnosis_Inflammatory',\n",
              "       'Diagnosis_Serrated', 'Diagnosis_Serrated, Hyperplastic',\n",
              "       'Diagnosis_T + V', 'Diagnosis_Traditional Serrated Adenoma',\n",
              "       'Diagnosis_Tubular', 'Diagnosis_Villous',\n",
              "       'Dysplasia_Grade_Differentiation_HGD',\n",
              "       'Dysplasia_Grade_Differentiation_LGD',\n",
              "       'Dysplasia_Grade_Differentiation_Mod Diff\\nOrigin: Tubulovillous',\n",
              "       'Dysplasia_Grade_Differentiation_Mod: Well Diff \\nOrigin: Tubulovillous',\n",
              "       'Dysplasia_Grade_Differentiation_T + V',\n",
              "       'Dysplasia_Grade_Differentiation_Well Diff'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the path where you want to save the file\n",
        "save_path = '/content/drive/My Drive/combined_data.csv'\n",
        "\n",
        "# Save the updated DataFrame to a CSV file\n",
        "combined_data.to_csv(save_path, index=False)\n",
        "\n",
        "print(f\"Dataset saved to {save_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wNU56I5h_Vjd",
        "outputId": "aeb0ec94-b7c1-4264-9f76-c619c7a98aac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset saved to /content/drive/My Drive/combined_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract relevant features\n",
        "morphological_features = [col for col in combined_data.columns if 'Polyp_Location_' in col or\n",
        "                          'Paris_' in col or\n",
        "                          'Pit_' in col or\n",
        "                          'LST_Type_' in col or\n",
        "                          'Diagnosis_' in col or\n",
        "                          'Dysplasia_Grade_' in col]\n",
        "\n",
        "# Include 'Size_cm' as it's essential for growth modeling\n",
        "selected_features = ['Patient_ID', 'Size_cm'] + morphological_features\n",
        "\n",
        "# Subset the dataset to selected features\n",
        "prepared_data = combined_data[selected_features]\n",
        "\n",
        "# Check the shape and columns of the prepared dataset\n",
        "print(f\"Prepared dataset contains {prepared_data.shape[0]} rows and {prepared_data.shape[1]} columns.\")\n",
        "prepared_data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "UVeAc3woMLU8",
        "outputId": "39b887c8-d3b3-4e37-a911-7b405ea42514"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prepared dataset contains 4727 rows and 65 columns.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Patient_ID   Size_cm  Polyp_Location_ Descending  Polyp_Location_ Sigmoid  \\\n",
              "0       23002  0.042553                           0                        0   \n",
              "1       23002  0.042553                           0                        0   \n",
              "2       23002  0.042553                           0                        0   \n",
              "3       23002  0.042553                           0                        0   \n",
              "4       23002  0.042553                           0                        0   \n",
              "\n",
              "   Polyp_Location_ Splenic Flexure  Polyp_Location_Anastomosis  \\\n",
              "0                                0                           0   \n",
              "1                                0                           0   \n",
              "2                                0                           0   \n",
              "3                                0                           0   \n",
              "4                                0                           0   \n",
              "\n",
              "   Polyp_Location_Asc / Transverse ?  Polyp_Location_Ascending  \\\n",
              "0                                  0                         0   \n",
              "1                                  0                         0   \n",
              "2                                  0                         0   \n",
              "3                                  0                         0   \n",
              "4                                  0                         0   \n",
              "\n",
              "   Polyp_Location_Cecum  Polyp_Location_Cecum\\nKissing Polyps  ...  \\\n",
              "0                     0                                     0  ...   \n",
              "1                     0                                     0  ...   \n",
              "2                     0                                     0  ...   \n",
              "3                     0                                     0  ...   \n",
              "4                     0                                     0  ...   \n",
              "\n",
              "   Diagnosis_T + V  Diagnosis_Traditional Serrated Adenoma  Diagnosis_Tubular  \\\n",
              "0                1                                       0                  0   \n",
              "1                1                                       0                  0   \n",
              "2                1                                       0                  0   \n",
              "3                1                                       0                  0   \n",
              "4                1                                       0                  0   \n",
              "\n",
              "   Diagnosis_Villous  Dysplasia_Grade_Differentiation_HGD  \\\n",
              "0                  0                                    0   \n",
              "1                  0                                    0   \n",
              "2                  0                                    0   \n",
              "3                  0                                    0   \n",
              "4                  0                                    0   \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_LGD  \\\n",
              "0                                    1   \n",
              "1                                    1   \n",
              "2                                    1   \n",
              "3                                    1   \n",
              "4                                    1   \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_Mod Diff\\nOrigin: Tubulovillous  \\\n",
              "0                                                  0                 \n",
              "1                                                  0                 \n",
              "2                                                  0                 \n",
              "3                                                  0                 \n",
              "4                                                  0                 \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_Mod: Well Diff \\nOrigin: Tubulovillous  \\\n",
              "0                                                  0                        \n",
              "1                                                  0                        \n",
              "2                                                  0                        \n",
              "3                                                  0                        \n",
              "4                                                  0                        \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_T + V  \\\n",
              "0                                      0   \n",
              "1                                      0   \n",
              "2                                      0   \n",
              "3                                      0   \n",
              "4                                      0   \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_Well Diff  \n",
              "0                                          0  \n",
              "1                                          0  \n",
              "2                                          0  \n",
              "3                                          0  \n",
              "4                                          0  \n",
              "\n",
              "[5 rows x 65 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-504ef32e-dccf-46fc-82e6-256ea5cf8e4b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>Size_cm</th>\n",
              "      <th>Polyp_Location_ Descending</th>\n",
              "      <th>Polyp_Location_ Sigmoid</th>\n",
              "      <th>Polyp_Location_ Splenic Flexure</th>\n",
              "      <th>Polyp_Location_Anastomosis</th>\n",
              "      <th>Polyp_Location_Asc / Transverse ?</th>\n",
              "      <th>Polyp_Location_Ascending</th>\n",
              "      <th>Polyp_Location_Cecum</th>\n",
              "      <th>Polyp_Location_Cecum\\nKissing Polyps</th>\n",
              "      <th>...</th>\n",
              "      <th>Diagnosis_T + V</th>\n",
              "      <th>Diagnosis_Traditional Serrated Adenoma</th>\n",
              "      <th>Diagnosis_Tubular</th>\n",
              "      <th>Diagnosis_Villous</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_HGD</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_LGD</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_Mod Diff\\nOrigin: Tubulovillous</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_Mod: Well Diff \\nOrigin: Tubulovillous</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_T + V</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_Well Diff</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23002</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>23002</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>23002</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>23002</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>23002</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 65 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-504ef32e-dccf-46fc-82e6-256ea5cf8e4b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-504ef32e-dccf-46fc-82e6-256ea5cf8e4b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-504ef32e-dccf-46fc-82e6-256ea5cf8e4b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1f3ee40f-0f80-4bec-8427-6371c2c58f66\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1f3ee40f-0f80-4bec-8427-6371c2c58f66')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1f3ee40f-0f80-4bec-8427-6371c2c58f66 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "prepared_data"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Target variable: Size_cm (continuous regression target)\n",
        "target_variable = 'Size_cm'\n",
        "\n",
        "# Morphological targets (classification tasks)\n",
        "morphological_targets = [col for col in combined_data.columns if 'Paris_' in col or 'Diagnosis_' in col]\n",
        "\n",
        "print(\"Target variable for regression:\", target_variable)\n",
        "print(\"Morphological classification targets:\", morphological_targets)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlU4KQB7OJLR",
        "outputId": "97856039-4f95-4813-adde-04f33dc4e675"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target variable for regression: Size_cm\n",
            "Morphological classification targets: ['Paris_0-IIa', 'Paris_0-IIa + IIc', 'Paris_0-IIa + Is', 'Paris_0-IIa /c', 'Paris_0-IIb', 'Paris_0-Ip', 'Paris_0-Ips', 'Paris_0-Is', 'Paris_0-lps', 'Paris_0-ls', 'Diagnosis_Adenocarcinoma', 'Diagnosis_Hyperplastic', 'Diagnosis_Inflammatory', 'Diagnosis_Serrated', 'Diagnosis_Serrated, Hyperplastic', 'Diagnosis_T + V', 'Diagnosis_Traditional Serrated Adenoma', 'Diagnosis_Tubular', 'Diagnosis_Villous']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters for simulation\n",
        "n_time_points = 5  # Number of time points\n",
        "growth_rate = 0.05  # Growth rate per time point\n",
        "max_size = 5.0  # Maximum size to prevent unrealistic growth\n",
        "\n",
        "# Initialize a list to store simulated data\n",
        "simulated_data = []\n",
        "\n",
        "# Loop through each unique Patient_ID\n",
        "for patient_id, group in prepared_data.groupby('Patient_ID'):\n",
        "    initial_row = group.iloc[0]  # Use the first record for each patient\n",
        "\n",
        "    # Create time steps for the patient\n",
        "    for t in range(n_time_points):\n",
        "        row = initial_row.copy()\n",
        "        row['Time_Point'] = t  # Add time point\n",
        "\n",
        "        # Simulate size growth\n",
        "        row['Size_cm'] = min(row['Size_cm'] * (1 + growth_rate * t), max_size)\n",
        "\n",
        "        # Simulate morphological changes (e.g., transitions in Paris classifications)\n",
        "        if t > 0:\n",
        "            paris_cols = [col for col in morphological_targets if 'Paris_' in col]\n",
        "            random_paris = np.random.choice(paris_cols, size=1)\n",
        "            row[random_paris[0]] = 1  # Assign a new Paris classification\n",
        "\n",
        "        # Append to the simulated dataset\n",
        "        simulated_data.append(row)\n",
        "\n",
        "# Combine simulated data into a new DataFrame\n",
        "simulated_data = pd.DataFrame(simulated_data)\n",
        "\n",
        "# Check the structure of the simulated data\n",
        "print(f\"Simulated dataset contains {simulated_data.shape[0]} rows and {simulated_data.shape[1]} columns.\")\n",
        "simulated_data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "JY92mPjTPkgY",
        "outputId": "09bee7d8-6deb-46cb-b08e-6d77a5da6af6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated dataset contains 515 rows and 66 columns.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Patient_ID   Size_cm  Polyp_Location_ Descending  Polyp_Location_ Sigmoid  \\\n",
              "0     23002.0  0.042553                         0.0                      0.0   \n",
              "0     23002.0  0.044681                         0.0                      0.0   \n",
              "0     23002.0  0.046809                         0.0                      0.0   \n",
              "0     23002.0  0.048936                         0.0                      0.0   \n",
              "0     23002.0  0.051064                         0.0                      0.0   \n",
              "\n",
              "   Polyp_Location_ Splenic Flexure  Polyp_Location_Anastomosis  \\\n",
              "0                              0.0                         0.0   \n",
              "0                              0.0                         0.0   \n",
              "0                              0.0                         0.0   \n",
              "0                              0.0                         0.0   \n",
              "0                              0.0                         0.0   \n",
              "\n",
              "   Polyp_Location_Asc / Transverse ?  Polyp_Location_Ascending  \\\n",
              "0                                0.0                       0.0   \n",
              "0                                0.0                       0.0   \n",
              "0                                0.0                       0.0   \n",
              "0                                0.0                       0.0   \n",
              "0                                0.0                       0.0   \n",
              "\n",
              "   Polyp_Location_Cecum  Polyp_Location_Cecum\\nKissing Polyps  ...  \\\n",
              "0                   0.0                                   0.0  ...   \n",
              "0                   0.0                                   0.0  ...   \n",
              "0                   0.0                                   0.0  ...   \n",
              "0                   0.0                                   0.0  ...   \n",
              "0                   0.0                                   0.0  ...   \n",
              "\n",
              "   Diagnosis_Traditional Serrated Adenoma  Diagnosis_Tubular  \\\n",
              "0                                     0.0                0.0   \n",
              "0                                     0.0                0.0   \n",
              "0                                     0.0                0.0   \n",
              "0                                     0.0                0.0   \n",
              "0                                     0.0                0.0   \n",
              "\n",
              "   Diagnosis_Villous  Dysplasia_Grade_Differentiation_HGD  \\\n",
              "0                0.0                                  0.0   \n",
              "0                0.0                                  0.0   \n",
              "0                0.0                                  0.0   \n",
              "0                0.0                                  0.0   \n",
              "0                0.0                                  0.0   \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_LGD  \\\n",
              "0                                  1.0   \n",
              "0                                  1.0   \n",
              "0                                  1.0   \n",
              "0                                  1.0   \n",
              "0                                  1.0   \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_Mod Diff\\nOrigin: Tubulovillous  \\\n",
              "0                                                0.0                 \n",
              "0                                                0.0                 \n",
              "0                                                0.0                 \n",
              "0                                                0.0                 \n",
              "0                                                0.0                 \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_Mod: Well Diff \\nOrigin: Tubulovillous  \\\n",
              "0                                                0.0                        \n",
              "0                                                0.0                        \n",
              "0                                                0.0                        \n",
              "0                                                0.0                        \n",
              "0                                                0.0                        \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_T + V  \\\n",
              "0                                    0.0   \n",
              "0                                    0.0   \n",
              "0                                    0.0   \n",
              "0                                    0.0   \n",
              "0                                    0.0   \n",
              "\n",
              "   Dysplasia_Grade_Differentiation_Well Diff  Time_Point  \n",
              "0                                        0.0         0.0  \n",
              "0                                        0.0         1.0  \n",
              "0                                        0.0         2.0  \n",
              "0                                        0.0         3.0  \n",
              "0                                        0.0         4.0  \n",
              "\n",
              "[5 rows x 66 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-468d6f60-fa66-46ad-8751-c0df678668be\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Patient_ID</th>\n",
              "      <th>Size_cm</th>\n",
              "      <th>Polyp_Location_ Descending</th>\n",
              "      <th>Polyp_Location_ Sigmoid</th>\n",
              "      <th>Polyp_Location_ Splenic Flexure</th>\n",
              "      <th>Polyp_Location_Anastomosis</th>\n",
              "      <th>Polyp_Location_Asc / Transverse ?</th>\n",
              "      <th>Polyp_Location_Ascending</th>\n",
              "      <th>Polyp_Location_Cecum</th>\n",
              "      <th>Polyp_Location_Cecum\\nKissing Polyps</th>\n",
              "      <th>...</th>\n",
              "      <th>Diagnosis_Traditional Serrated Adenoma</th>\n",
              "      <th>Diagnosis_Tubular</th>\n",
              "      <th>Diagnosis_Villous</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_HGD</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_LGD</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_Mod Diff\\nOrigin: Tubulovillous</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_Mod: Well Diff \\nOrigin: Tubulovillous</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_T + V</th>\n",
              "      <th>Dysplasia_Grade_Differentiation_Well Diff</th>\n",
              "      <th>Time_Point</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23002.0</td>\n",
              "      <td>0.042553</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23002.0</td>\n",
              "      <td>0.044681</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23002.0</td>\n",
              "      <td>0.046809</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23002.0</td>\n",
              "      <td>0.048936</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>23002.0</td>\n",
              "      <td>0.051064</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 66 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-468d6f60-fa66-46ad-8751-c0df678668be')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-468d6f60-fa66-46ad-8751-c0df678668be button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-468d6f60-fa66-46ad-8751-c0df678668be');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ee1b6d77-f70d-49df-be87-cfc5c7279528\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ee1b6d77-f70d-49df-be87-cfc5c7279528')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ee1b6d77-f70d-49df-be87-cfc5c7279528 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "simulated_data"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the simulated dataset for later use\n",
        "output_path = '/content/drive/MyDrive/simulated_data.csv'\n",
        "simulated_data.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"Simulated dataset saved at {output_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deATsh_BQKDq",
        "outputId": "c482aedf-3bc5-48c1-e7fa-d31c44a5d6e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Simulated dataset saved at /content/drive/MyDrive/simulated_data.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "# Step 2.1: Splitting the Dataset\n",
        "# Define the target variable and morphological classification targets\n",
        "target_variable = 'Size_cm'\n",
        "morphological_targets = [col for col in simulated_data.columns if 'Paris_' in col or 'Diagnosis_' in col]\n",
        "\n",
        "# Add lagged features for Size_cm\n",
        "def add_lag_features(df, target, lag_steps=2):\n",
        "    for lag in range(1, lag_steps + 1):\n",
        "        df[f\"{target}_t-{lag}\"] = df[target].shift(lag, fill_value=0)\n",
        "    return df\n",
        "\n",
        "# Add lag features to the dataset\n",
        "simulated_data_with_lags = simulated_data.copy()\n",
        "simulated_data_with_lags = add_lag_features(simulated_data_with_lags, target=\"Size_cm\", lag_steps=2)\n",
        "\n",
        "# Define features (X) and targets (y)\n",
        "X = simulated_data_with_lags.drop(columns=[target_variable] + morphological_targets)\n",
        "y_size = simulated_data_with_lags[target_variable]\n",
        "y_morphology = simulated_data_with_lags[morphological_targets]\n",
        "\n",
        "# Perform a train-test split for size prediction and morphology classification\n",
        "X_train, X_temp, y_size_train, y_size_temp, y_morphology_train, y_morphology_temp = train_test_split(\n",
        "    X, y_size, y_morphology, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "X_val, X_test, y_size_val, y_size_test, y_morphology_val, y_morphology_test = train_test_split(\n",
        "    X_temp, y_size_temp, y_morphology_temp, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2.2: Feature Engineering\n",
        "# Scale the features\n",
        "#scaler = StandardScaler()\n",
        "X_train_scaled = X_train\n",
        "X_val_scaled = X_val\n",
        "X_test_scaled = X_test\n",
        "# Display the updated training set structure\n",
        "print(\"Updated dataset splits after scaling and adding lag features:\")\n",
        "print(f\"Training set: X_train_scaled: {X_train_scaled.shape}, y_size_train: {y_size_train.shape}, y_morphology_train: {y_morphology_train.shape}\")\n",
        "print(f\"Validation set: X_val_scaled: {X_val_scaled.shape}, y_size_val: {y_size_val.shape}, y_morphology_val: {y_morphology_val.shape}\")\n",
        "print(f\"Test set: X_test_scaled: {X_test_scaled.shape}, y_size_test: {y_size_test.shape}, y_morphology_test: {y_morphology_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIadW0xsQcxe",
        "outputId": "f64fc688-5361-47c5-bc38-336210c21824"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated dataset splits after scaling and adding lag features:\n",
            "Training set: X_train_scaled: (360, 48), y_size_train: (360,), y_morphology_train: (360, 19)\n",
            "Validation set: X_val_scaled: (77, 48), y_size_val: (77,), y_morphology_val: (77, 19)\n",
            "Test set: X_test_scaled: (78, 48), y_size_test: (78,), y_morphology_test: (78, 19)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data into sequences for LSTM\n",
        "def create_sequences(features, target, time_steps=3):\n",
        "    X, y = [], []\n",
        "    for i in range(len(features) - time_steps):\n",
        "        # Create a sequence of features\n",
        "        X.append(features.iloc[i: i + time_steps].values)\n",
        "        # Use the target corresponding to the last time step in the sequence\n",
        "        y.append(target.iloc[i + time_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create sequences for regression (Size_cm)\n",
        "time_steps = 3\n",
        "X_train_seq, y_size_train_seq = create_sequences(X_train_scaled, y_size_train, time_steps)\n",
        "X_val_seq, y_size_val_seq = create_sequences(X_val_scaled, y_size_val, time_steps)\n",
        "X_test_seq, y_size_test_seq = create_sequences(X_test_scaled, y_size_test, time_steps)\n",
        "\n",
        "print(f\"Training data shape (Regression): {X_train_seq.shape}, {y_size_train_seq.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X26FQCEyR5Jj",
        "outputId": "6c8311d5-321c-4f32-b8f4-180e35321217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape (Regression): (357, 3, 48), (357,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Define the LSTM model\n",
        "lstm_model = Sequential([\n",
        "    LSTM(64, activation='relu', input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(1)  # Output layer for regression\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "lstm_model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
        "\n",
        "# Train the model\n",
        "history = lstm_model.fit(\n",
        "    X_train_seq, y_size_train_seq,\n",
        "    validation_data=(X_val_seq, y_size_val_seq),\n",
        "    epochs=500, batch_size=16, verbose=1\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "lstm_eval = lstm_model.evaluate(X_test_seq, y_size_test_seq, verbose=0)\n",
        "print(f\"LSTM Regression Model Test Loss: {lstm_eval[0]}, Test MAE: {lstm_eval[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPHgTNjNSWHV",
        "outputId": "8c196c96-2687-4290-8721-ff9a42d1a87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 91ms/step - loss: 366549.5938 - mae: 469.0813 - val_loss: 671.0646 - val_mae: 24.3196\n",
            "Epoch 2/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 105268.6328 - mae: 238.4593 - val_loss: 1082.4766 - val_mae: 32.8205\n",
            "Epoch 3/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66706.6562 - mae: 190.1884 - val_loss: 541.7000 - val_mae: 23.0418\n",
            "Epoch 4/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 56179.9531 - mae: 172.1221 - val_loss: 341.0141 - val_mae: 18.1963\n",
            "Epoch 5/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35332.3711 - mae: 137.4743 - val_loss: 12.8247 - val_mae: 3.2856\n",
            "Epoch 6/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 26326.7793 - mae: 115.5117 - val_loss: 320.4519 - val_mae: 17.8462\n",
            "Epoch 7/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17284.8672 - mae: 92.8680 - val_loss: 31.5049 - val_mae: 5.5720\n",
            "Epoch 8/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14599.2178 - mae: 76.5379 - val_loss: 1.4820 - val_mae: 0.9544\n",
            "Epoch 9/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15606.0488 - mae: 79.5692 - val_loss: 8.4830 - val_mae: 2.9051\n",
            "Epoch 10/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10185.4990 - mae: 69.5780 - val_loss: 0.1349 - val_mae: 0.3211\n",
            "Epoch 11/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8037.2432 - mae: 59.9464 - val_loss: 0.3194 - val_mae: 0.4091\n",
            "Epoch 12/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9704.9189 - mae: 64.1770 - val_loss: 1.5338 - val_mae: 1.2015\n",
            "Epoch 13/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8106.9341 - mae: 59.1901 - val_loss: 52.0744 - val_mae: 7.2099\n",
            "Epoch 14/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7392.0576 - mae: 56.5391 - val_loss: 21.3626 - val_mae: 4.6048\n",
            "Epoch 15/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7339.2427 - mae: 55.8498 - val_loss: 2.4808 - val_mae: 1.5352\n",
            "Epoch 16/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5315.0396 - mae: 48.7436 - val_loss: 1.7603 - val_mae: 1.2720\n",
            "Epoch 17/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4115.1138 - mae: 43.6014 - val_loss: 8.4583 - val_mae: 2.8969\n",
            "Epoch 18/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4450.3037 - mae: 40.4327 - val_loss: 1.1105 - val_mae: 1.0383\n",
            "Epoch 19/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5238.8721 - mae: 42.7298 - val_loss: 18.4677 - val_mae: 4.2828\n",
            "Epoch 20/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3953.7480 - mae: 40.0172 - val_loss: 0.9548 - val_mae: 0.9000\n",
            "Epoch 21/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3314.7256 - mae: 34.5852 - val_loss: 0.4444 - val_mae: 0.5762\n",
            "Epoch 22/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2779.8142 - mae: 33.4337 - val_loss: 0.7134 - val_mae: 0.7484\n",
            "Epoch 23/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4152.1084 - mae: 38.5144 - val_loss: 0.2324 - val_mae: 0.4246\n",
            "Epoch 24/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3621.7417 - mae: 33.6281 - val_loss: 0.4306 - val_mae: 0.5593\n",
            "Epoch 25/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2464.2214 - mae: 30.9859 - val_loss: 6.5100 - val_mae: 2.5227\n",
            "Epoch 26/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1983.8964 - mae: 29.7103 - val_loss: 7.8881 - val_mae: 2.7980\n",
            "Epoch 27/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2207.7051 - mae: 29.2318 - val_loss: 11.7437 - val_mae: 3.3933\n",
            "Epoch 28/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1751.3090 - mae: 27.2747 - val_loss: 26.9037 - val_mae: 5.1832\n",
            "Epoch 29/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2452.4048 - mae: 29.3428 - val_loss: 27.9456 - val_mae: 5.2823\n",
            "Epoch 30/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1945.8368 - mae: 25.5838 - val_loss: 23.8531 - val_mae: 4.8806\n",
            "Epoch 31/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2096.8918 - mae: 27.6091 - val_loss: 23.5699 - val_mae: 4.8500\n",
            "Epoch 32/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1404.7272 - mae: 23.5704 - val_loss: 14.8095 - val_mae: 3.8422\n",
            "Epoch 33/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1075.0754 - mae: 20.0568 - val_loss: 10.5364 - val_mae: 3.2384\n",
            "Epoch 34/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1372.4242 - mae: 21.1200 - val_loss: 15.5012 - val_mae: 3.9310\n",
            "Epoch 35/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1409.8618 - mae: 21.5222 - val_loss: 15.0897 - val_mae: 3.8781\n",
            "Epoch 36/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 836.7142 - mae: 18.7604 - val_loss: 8.5857 - val_mae: 2.9215\n",
            "Epoch 37/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1535.7640 - mae: 21.1402 - val_loss: 11.1299 - val_mae: 3.3255\n",
            "Epoch 38/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1147.7601 - mae: 18.9868 - val_loss: 12.0512 - val_mae: 3.4649\n",
            "Epoch 39/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 568.4820 - mae: 15.6048 - val_loss: 7.0661 - val_mae: 2.6491\n",
            "Epoch 40/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 994.5815 - mae: 17.1199 - val_loss: 5.8572 - val_mae: 2.4102\n",
            "Epoch 41/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 999.4942 - mae: 18.8895 - val_loss: 3.0377 - val_mae: 1.7286\n",
            "Epoch 42/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 22060.8945 - mae: 71.2839 - val_loss: 1363.0254 - val_mae: 36.8817\n",
            "Epoch 43/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5918.2969 - mae: 43.7786 - val_loss: 22.1032 - val_mae: 4.6522\n",
            "Epoch 44/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 834.1949 - mae: 19.9951 - val_loss: 5.7650 - val_mae: 2.3125\n",
            "Epoch 45/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 615.4747 - mae: 15.8830 - val_loss: 0.4436 - val_mae: 0.5522\n",
            "Epoch 46/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 307.6281 - mae: 11.5909 - val_loss: 1.2362 - val_mae: 1.0688\n",
            "Epoch 47/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 540.4662 - mae: 14.1932 - val_loss: 1.5834 - val_mae: 1.2334\n",
            "Epoch 48/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 575.8768 - mae: 12.8867 - val_loss: 0.9748 - val_mae: 0.9559\n",
            "Epoch 49/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 753.3895 - mae: 12.8711 - val_loss: 1.2657 - val_mae: 1.0978\n",
            "Epoch 50/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 618.5690 - mae: 13.1974 - val_loss: 1.5038 - val_mae: 1.2018\n",
            "Epoch 51/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 693.1102 - mae: 14.0938 - val_loss: 1.8162 - val_mae: 1.3266\n",
            "Epoch 52/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 574.0643 - mae: 11.9018 - val_loss: 2.5705 - val_mae: 1.5858\n",
            "Epoch 53/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 396.2768 - mae: 10.7348 - val_loss: 2.3076 - val_mae: 1.5010\n",
            "Epoch 54/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 222.8552 - mae: 8.4381 - val_loss: 2.1615 - val_mae: 1.4515\n",
            "Epoch 55/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 286.4890 - mae: 9.5155 - val_loss: 3.1849 - val_mae: 1.7695\n",
            "Epoch 56/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 339.6537 - mae: 10.3683 - val_loss: 3.1270 - val_mae: 1.7538\n",
            "Epoch 57/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 383.6529 - mae: 10.5026 - val_loss: 3.4313 - val_mae: 1.8392\n",
            "Epoch 58/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 207.2803 - mae: 8.2612 - val_loss: 3.8852 - val_mae: 1.9588\n",
            "Epoch 59/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 365.6363 - mae: 9.0440 - val_loss: 2.7331 - val_mae: 1.6395\n",
            "Epoch 60/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 332.9564 - mae: 8.5260 - val_loss: 1.8589 - val_mae: 1.3472\n",
            "Epoch 61/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 297.2767 - mae: 9.2418 - val_loss: 2.2975 - val_mae: 1.5014\n",
            "Epoch 62/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 151.8865 - mae: 7.0367 - val_loss: 3.0258 - val_mae: 1.7271\n",
            "Epoch 63/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 245.8687 - mae: 7.7228 - val_loss: 3.6956 - val_mae: 1.9113\n",
            "Epoch 64/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 316.5336 - mae: 8.6667 - val_loss: 3.9715 - val_mae: 1.9824\n",
            "Epoch 65/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 411.0498 - mae: 8.9629 - val_loss: 2.9308 - val_mae: 1.7000\n",
            "Epoch 66/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 293.4216 - mae: 8.0474 - val_loss: 1.6361 - val_mae: 1.2635\n",
            "Epoch 67/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 309.6758 - mae: 8.2311 - val_loss: 1.8107 - val_mae: 1.3309\n",
            "Epoch 68/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 261.1885 - mae: 8.2627 - val_loss: 2.3556 - val_mae: 1.5219\n",
            "Epoch 69/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 124.0852 - mae: 6.2358 - val_loss: 2.8342 - val_mae: 1.6717\n",
            "Epoch 70/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 160.0532 - mae: 6.7720 - val_loss: 3.2491 - val_mae: 1.7916\n",
            "Epoch 71/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 228.6664 - mae: 7.6013 - val_loss: 2.8232 - val_mae: 1.6687\n",
            "Epoch 72/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 202.1307 - mae: 6.7900 - val_loss: 2.9746 - val_mae: 1.7135\n",
            "Epoch 73/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 266.4645 - mae: 6.8625 - val_loss: 2.6180 - val_mae: 1.6062\n",
            "Epoch 74/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106.5308 - mae: 5.7302 - val_loss: 3.2506 - val_mae: 1.7923\n",
            "Epoch 75/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 244.7628 - mae: 8.2011 - val_loss: 2.9761 - val_mae: 1.7141\n",
            "Epoch 76/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 85.7820 - mae: 4.6732 - val_loss: 2.6096 - val_mae: 1.6038\n",
            "Epoch 77/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 223.9592 - mae: 6.8199 - val_loss: 2.9863 - val_mae: 1.7173\n",
            "Epoch 78/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 142.2770 - mae: 6.4735 - val_loss: 2.7408 - val_mae: 1.6444\n",
            "Epoch 79/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 164.7858 - mae: 6.6469 - val_loss: 2.3497 - val_mae: 1.5209\n",
            "Epoch 80/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 392.4051 - mae: 7.8109 - val_loss: 2.1041 - val_mae: 1.4380\n",
            "Epoch 81/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 322.8412 - mae: 7.3733 - val_loss: 2.3432 - val_mae: 1.5200\n",
            "Epoch 82/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 89.1371 - mae: 4.7986 - val_loss: 2.5898 - val_mae: 1.6000\n",
            "Epoch 83/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 71.8487 - mae: 4.7345 - val_loss: 2.5207 - val_mae: 1.5786\n",
            "Epoch 84/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 132.8969 - mae: 5.9029 - val_loss: 2.2330 - val_mae: 1.4856\n",
            "Epoch 85/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 106.2183 - mae: 5.1317 - val_loss: 2.0469 - val_mae: 1.4222\n",
            "Epoch 86/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 148.2983 - mae: 5.9918 - val_loss: 1.6937 - val_mae: 1.2921\n",
            "Epoch 87/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 195.6955 - mae: 5.7077 - val_loss: 1.5991 - val_mae: 1.2550\n",
            "Epoch 88/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 74.8771 - mae: 4.5405 - val_loss: 1.6588 - val_mae: 1.2786\n",
            "Epoch 89/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 160.5082 - mae: 5.4434 - val_loss: 1.6974 - val_mae: 1.2935\n",
            "Epoch 90/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 170.7733 - mae: 5.7393 - val_loss: 1.8706 - val_mae: 1.3588\n",
            "Epoch 91/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 108.6045 - mae: 4.9472 - val_loss: 1.5061 - val_mae: 1.2174\n",
            "Epoch 92/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 209.8071 - mae: 6.1652 - val_loss: 1.5338 - val_mae: 1.2287\n",
            "Epoch 93/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 95.2608 - mae: 4.5273 - val_loss: 1.2256 - val_mae: 1.0961\n",
            "Epoch 94/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 75.6779 - mae: 4.6370 - val_loss: 1.1309 - val_mae: 1.0520\n",
            "Epoch 95/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106.0869 - mae: 4.6565 - val_loss: 0.8545 - val_mae: 0.9113\n",
            "Epoch 96/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 108.9206 - mae: 4.3409 - val_loss: 0.9938 - val_mae: 0.9847\n",
            "Epoch 97/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 95.0069 - mae: 4.2478 - val_loss: 1.0701 - val_mae: 1.0227\n",
            "Epoch 98/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 100.1058 - mae: 4.0128 - val_loss: 0.9658 - val_mae: 0.9704\n",
            "Epoch 99/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 49.7515 - mae: 3.1353 - val_loss: 0.9836 - val_mae: 0.9796\n",
            "Epoch 100/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 127.1266 - mae: 4.8037 - val_loss: 0.9190 - val_mae: 0.9460\n",
            "Epoch 101/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 70.5914 - mae: 4.0315 - val_loss: 0.7153 - val_mae: 0.8314\n",
            "Epoch 102/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 97.2964 - mae: 4.1473 - val_loss: 0.6554 - val_mae: 0.7946\n",
            "Epoch 103/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 48.1014 - mae: 3.3730 - val_loss: 0.6610 - val_mae: 0.7981\n",
            "Epoch 104/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 80.3786 - mae: 3.7859 - val_loss: 0.6600 - val_mae: 0.7975\n",
            "Epoch 105/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 92.3357 - mae: 4.0898 - val_loss: 0.6445 - val_mae: 0.7877\n",
            "Epoch 106/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 39.7007 - mae: 2.8955 - val_loss: 0.6992 - val_mae: 0.8217\n",
            "Epoch 107/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 66.1016 - mae: 4.1340 - val_loss: 0.5099 - val_mae: 0.6971\n",
            "Epoch 108/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 29.8165 - mae: 2.6195 - val_loss: 0.4527 - val_mae: 0.6548\n",
            "Epoch 109/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 106.7076 - mae: 4.2342 - val_loss: 0.6437 - val_mae: 0.7871\n",
            "Epoch 110/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.8902 - mae: 3.7715 - val_loss: 0.7980 - val_mae: 0.8797\n",
            "Epoch 111/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 54.4383 - mae: 3.1261 - val_loss: 0.7433 - val_mae: 0.8481\n",
            "Epoch 112/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 61.7274 - mae: 3.4149 - val_loss: 0.6243 - val_mae: 0.7747\n",
            "Epoch 113/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 91.6740 - mae: 4.1213 - val_loss: 0.5699 - val_mae: 0.7389\n",
            "Epoch 114/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 30.0677 - mae: 2.5357 - val_loss: 0.6280 - val_mae: 0.7771\n",
            "Epoch 115/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 98.0070 - mae: 3.7443 - val_loss: 0.4936 - val_mae: 0.6852\n",
            "Epoch 116/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 200.4242 - mae: 4.2922 - val_loss: 0.2307 - val_mae: 0.4546\n",
            "Epoch 117/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 139.4085 - mae: 4.0824 - val_loss: 0.1937 - val_mae: 0.4119\n",
            "Epoch 118/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 72.1525 - mae: 3.3624 - val_loss: 0.3132 - val_mae: 0.5377\n",
            "Epoch 119/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 38.1169 - mae: 2.4210 - val_loss: 0.3580 - val_mae: 0.5779\n",
            "Epoch 120/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 47.0670 - mae: 2.9741 - val_loss: 0.3564 - val_mae: 0.5765\n",
            "Epoch 121/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 121.4398 - mae: 3.7076 - val_loss: 0.2677 - val_mae: 0.4936\n",
            "Epoch 122/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.0312 - mae: 2.4646 - val_loss: 0.2865 - val_mae: 0.5123\n",
            "Epoch 123/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 69.6221 - mae: 3.0993 - val_loss: 0.2913 - val_mae: 0.5169\n",
            "Epoch 124/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 116.7106 - mae: 3.4472 - val_loss: 0.2490 - val_mae: 0.4743\n",
            "Epoch 125/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 39.6902 - mae: 2.4138 - val_loss: 0.2381 - val_mae: 0.4626\n",
            "Epoch 126/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 22.5410 - mae: 1.9037 - val_loss: 0.2343 - val_mae: 0.4585\n",
            "Epoch 127/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 20.6860 - mae: 2.1105 - val_loss: 0.1732 - val_mae: 0.3862\n",
            "Epoch 128/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22.2848 - mae: 1.7943 - val_loss: 0.2054 - val_mae: 0.4259\n",
            "Epoch 129/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.9513 - mae: 2.5484 - val_loss: 0.1802 - val_mae: 0.3951\n",
            "Epoch 130/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.0478 - mae: 2.1473 - val_loss: 0.1561 - val_mae: 0.3634\n",
            "Epoch 131/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 42.4692 - mae: 2.3678 - val_loss: 0.1492 - val_mae: 0.3538\n",
            "Epoch 132/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 42.7564 - mae: 2.2676 - val_loss: 0.1688 - val_mae: 0.3804\n",
            "Epoch 133/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 35.5685 - mae: 2.0066 - val_loss: 0.1714 - val_mae: 0.3839\n",
            "Epoch 134/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.9636 - mae: 2.2655 - val_loss: 0.2133 - val_mae: 0.4350\n",
            "Epoch 135/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22.4433 - mae: 1.9072 - val_loss: 0.1941 - val_mae: 0.4123\n",
            "Epoch 136/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 37.8394 - mae: 1.8794 - val_loss: 0.1452 - val_mae: 0.3481\n",
            "Epoch 137/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.7872 - mae: 1.7431 - val_loss: 0.1185 - val_mae: 0.3074\n",
            "Epoch 138/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.8134 - mae: 1.5911 - val_loss: 0.1525 - val_mae: 0.3584\n",
            "Epoch 139/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 30.7188 - mae: 1.9510 - val_loss: 0.1309 - val_mae: 0.3268\n",
            "Epoch 140/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.4223 - mae: 1.7475 - val_loss: 0.1290 - val_mae: 0.3239\n",
            "Epoch 141/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.0998 - mae: 1.5537 - val_loss: 0.1670 - val_mae: 0.3780\n",
            "Epoch 142/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 37.4859 - mae: 2.0363 - val_loss: 0.1665 - val_mae: 0.3774\n",
            "Epoch 143/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.2079 - mae: 1.5939 - val_loss: 0.2373 - val_mae: 0.4618\n",
            "Epoch 144/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.9126 - mae: 1.6844 - val_loss: 0.2087 - val_mae: 0.4297\n",
            "Epoch 145/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 32.4010 - mae: 2.0971 - val_loss: 0.1245 - val_mae: 0.3169\n",
            "Epoch 146/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 19.4654 - mae: 1.8009 - val_loss: 0.1213 - val_mae: 0.3118\n",
            "Epoch 147/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 28.8475 - mae: 2.0613 - val_loss: 0.1389 - val_mae: 0.3389\n",
            "Epoch 148/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.8942 - mae: 1.6908 - val_loss: 0.1457 - val_mae: 0.3487\n",
            "Epoch 149/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 32.9948 - mae: 1.7839 - val_loss: 0.1180 - val_mae: 0.3065\n",
            "Epoch 150/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 21.2712 - mae: 1.6963 - val_loss: 0.0718 - val_mae: 0.2183\n",
            "Epoch 151/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 48.2223 - mae: 2.0552 - val_loss: 0.1133 - val_mae: 0.2987\n",
            "Epoch 152/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.7415 - mae: 1.3327 - val_loss: 0.0837 - val_mae: 0.2439\n",
            "Epoch 153/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45.9049 - mae: 2.0451 - val_loss: 0.0769 - val_mae: 0.2292\n",
            "Epoch 154/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.3039 - mae: 1.3355 - val_loss: 0.1255 - val_mae: 0.3183\n",
            "Epoch 155/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 19.3001 - mae: 1.4088 - val_loss: 0.1513 - val_mae: 0.3566\n",
            "Epoch 156/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 17.8390 - mae: 1.7063 - val_loss: 0.1559 - val_mae: 0.3631\n",
            "Epoch 157/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2275 - mae: 1.0425 - val_loss: 0.1366 - val_mae: 0.3353\n",
            "Epoch 158/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 31.2508 - mae: 1.7037 - val_loss: 0.1452 - val_mae: 0.3481\n",
            "Epoch 159/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 25.7455 - mae: 1.6617 - val_loss: 0.1782 - val_mae: 0.3926\n",
            "Epoch 160/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 18.1595 - mae: 1.3680 - val_loss: 0.2003 - val_mae: 0.4198\n",
            "Epoch 161/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 16.4800 - mae: 1.4894 - val_loss: 0.1093 - val_mae: 0.2919\n",
            "Epoch 162/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 23.6301 - mae: 1.5661 - val_loss: 0.0807 - val_mae: 0.2379\n",
            "Epoch 163/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.1324 - mae: 1.0249 - val_loss: 0.1039 - val_mae: 0.2821\n",
            "Epoch 164/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5357 - mae: 1.0806 - val_loss: 0.1672 - val_mae: 0.3781\n",
            "Epoch 165/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.6415 - mae: 1.2624 - val_loss: 0.1693 - val_mae: 0.3811\n",
            "Epoch 166/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.9013 - mae: 1.0937 - val_loss: 0.1298 - val_mae: 0.3252\n",
            "Epoch 167/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.7412 - mae: 1.2682 - val_loss: 0.1247 - val_mae: 0.3173\n",
            "Epoch 168/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 10.8014 - mae: 1.0842 - val_loss: 0.1019 - val_mae: 0.2790\n",
            "Epoch 169/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7759 - mae: 0.9551 - val_loss: 0.1059 - val_mae: 0.2861\n",
            "Epoch 170/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 33.1810 - mae: 1.2768 - val_loss: 0.0827 - val_mae: 0.2422\n",
            "Epoch 171/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 21.2214 - mae: 1.6829 - val_loss: 0.0524 - val_mae: 0.1685\n",
            "Epoch 172/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 29.6773 - mae: 1.1792 - val_loss: 0.0441 - val_mae: 0.1416\n",
            "Epoch 173/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 22.0192 - mae: 0.9934 - val_loss: 0.0856 - val_mae: 0.2484\n",
            "Epoch 174/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 7.0641 - mae: 0.9078 - val_loss: 0.0833 - val_mae: 0.2435\n",
            "Epoch 175/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6759 - mae: 0.8969 - val_loss: 0.0635 - val_mae: 0.1986\n",
            "Epoch 176/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5014 - mae: 1.0131 - val_loss: 0.0590 - val_mae: 0.1870\n",
            "Epoch 177/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.4403 - mae: 1.1285 - val_loss: 0.0735 - val_mae: 0.2225\n",
            "Epoch 178/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.5763 - mae: 0.8757 - val_loss: 0.0574 - val_mae: 0.1827\n",
            "Epoch 179/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 14.2030 - mae: 1.0375 - val_loss: 0.0710 - val_mae: 0.2170\n",
            "Epoch 180/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.8212 - mae: 1.0775 - val_loss: 0.0634 - val_mae: 0.1985\n",
            "Epoch 181/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 138.6632 - mae: 2.2428 - val_loss: 0.0531 - val_mae: 0.1704\n",
            "Epoch 182/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.0195 - mae: 0.9912 - val_loss: 0.0592 - val_mae: 0.1878\n",
            "Epoch 183/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.2982 - mae: 1.0737 - val_loss: 0.0371 - val_mae: 0.1143\n",
            "Epoch 184/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4219 - mae: 0.6214 - val_loss: 0.0469 - val_mae: 0.1513\n",
            "Epoch 185/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 12.9832 - mae: 1.0630 - val_loss: 0.0509 - val_mae: 0.1641\n",
            "Epoch 186/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.2033 - mae: 1.0295 - val_loss: 0.0475 - val_mae: 0.1535\n",
            "Epoch 187/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.7528 - mae: 1.1386 - val_loss: 0.0342 - val_mae: 0.1010\n",
            "Epoch 188/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1578 - mae: 0.5934 - val_loss: 0.0411 - val_mae: 0.1308\n",
            "Epoch 189/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 27.9580 - mae: 0.9861 - val_loss: 0.0542 - val_mae: 0.1736\n",
            "Epoch 190/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 75.3376 - mae: 1.3018 - val_loss: 0.0360 - val_mae: 0.1087\n",
            "Epoch 191/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.7548 - mae: 0.8251 - val_loss: 0.0464 - val_mae: 0.1494\n",
            "Epoch 192/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 11.5271 - mae: 0.8539 - val_loss: 0.0385 - val_mae: 0.1205\n",
            "Epoch 193/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 13.5398 - mae: 0.9102 - val_loss: 0.0442 - val_mae: 0.1419\n",
            "Epoch 194/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.1477 - mae: 1.1396 - val_loss: 0.0646 - val_mae: 0.2016\n",
            "Epoch 195/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.0825 - mae: 0.9204 - val_loss: 0.0422 - val_mae: 0.1349\n",
            "Epoch 196/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 15.4285 - mae: 1.0032 - val_loss: 0.0497 - val_mae: 0.1602\n",
            "Epoch 197/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6512 - mae: 0.8183 - val_loss: 0.0381 - val_mae: 0.1189\n",
            "Epoch 198/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3895 - mae: 0.6029 - val_loss: 0.0257 - val_mae: 0.0531\n",
            "Epoch 199/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6976 - mae: 0.7336 - val_loss: 0.0241 - val_mae: 0.0737\n",
            "Epoch 200/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.4442 - mae: 0.9630 - val_loss: 0.0270 - val_mae: 0.0600\n",
            "Epoch 201/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7478 - mae: 0.6900 - val_loss: 0.0486 - val_mae: 0.1569\n",
            "Epoch 202/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 8.3885 - mae: 0.9042 - val_loss: 0.0403 - val_mae: 0.1274\n",
            "Epoch 203/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.8882 - mae: 0.8070 - val_loss: 0.0617 - val_mae: 0.1941\n",
            "Epoch 204/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5525 - mae: 0.7289 - val_loss: 0.0613 - val_mae: 0.1931\n",
            "Epoch 205/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 15.4559 - mae: 1.1491 - val_loss: 0.0342 - val_mae: 0.1003\n",
            "Epoch 206/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9862 - mae: 0.5122 - val_loss: 0.0456 - val_mae: 0.1468\n",
            "Epoch 207/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.5235 - mae: 0.9105 - val_loss: 0.0455 - val_mae: 0.1464\n",
            "Epoch 208/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.1269 - mae: 0.6311 - val_loss: 0.0281 - val_mae: 0.0662\n",
            "Epoch 209/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5369 - mae: 0.5235 - val_loss: 0.0338 - val_mae: 0.0986\n",
            "Epoch 210/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.5492 - mae: 0.8426 - val_loss: 0.0289 - val_mae: 0.0717\n",
            "Epoch 211/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7104 - mae: 0.5642 - val_loss: 0.0385 - val_mae: 0.1204\n",
            "Epoch 212/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6018 - mae: 0.4391 - val_loss: 0.0342 - val_mae: 0.1006\n",
            "Epoch 213/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.4551 - mae: 0.7319 - val_loss: 0.0555 - val_mae: 0.1774\n",
            "Epoch 214/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 40.8076 - mae: 0.9944 - val_loss: 0.0339 - val_mae: 0.0990\n",
            "Epoch 215/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1338 - mae: 0.4019 - val_loss: 0.0380 - val_mae: 0.1182\n",
            "Epoch 216/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 36.8969 - mae: 1.0066 - val_loss: 0.0417 - val_mae: 0.1328\n",
            "Epoch 217/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.5762 - mae: 0.8027 - val_loss: 0.0246 - val_mae: 0.0635\n",
            "Epoch 218/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 13.4572 - mae: 0.7809 - val_loss: 0.0562 - val_mae: 0.1792\n",
            "Epoch 219/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1847 - mae: 0.5902 - val_loss: 0.0282 - val_mae: 0.0667\n",
            "Epoch 220/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1890 - mae: 0.4216 - val_loss: 0.0340 - val_mae: 0.0997\n",
            "Epoch 221/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.6926 - mae: 0.6975 - val_loss: 0.0329 - val_mae: 0.0939\n",
            "Epoch 222/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.1992 - mae: 0.6094 - val_loss: 0.0340 - val_mae: 0.0999\n",
            "Epoch 223/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 15.0336 - mae: 0.9075 - val_loss: 0.0242 - val_mae: 0.0866\n",
            "Epoch 224/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0836 - mae: 0.3797 - val_loss: 0.0531 - val_mae: 0.1706\n",
            "Epoch 225/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0371 - mae: 0.7362 - val_loss: 0.0264 - val_mae: 0.0532\n",
            "Epoch 226/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4685 - mae: 0.4838 - val_loss: 0.0290 - val_mae: 0.0718\n",
            "Epoch 227/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.4356 - mae: 0.7405 - val_loss: 0.0269 - val_mae: 0.0574\n",
            "Epoch 228/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1437 - mae: 0.4177 - val_loss: 0.0317 - val_mae: 0.0870\n",
            "Epoch 229/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.3410 - mae: 0.5117 - val_loss: 0.0528 - val_mae: 0.1696\n",
            "Epoch 230/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.2064 - mae: 1.0181 - val_loss: 0.0750 - val_mae: 0.2683\n",
            "Epoch 231/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.7618 - mae: 0.7640 - val_loss: 0.0840 - val_mae: 0.2449\n",
            "Epoch 232/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.1200 - mae: 0.5691 - val_loss: 0.0343 - val_mae: 0.1006\n",
            "Epoch 233/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 22.1688 - mae: 0.9943 - val_loss: 0.0299 - val_mae: 0.0772\n",
            "Epoch 234/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7880 - mae: 0.3384 - val_loss: 0.0371 - val_mae: 0.1138\n",
            "Epoch 235/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5066 - mae: 0.5069 - val_loss: 0.0260 - val_mae: 0.0512\n",
            "Epoch 236/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9136 - mae: 0.2869 - val_loss: 0.0270 - val_mae: 0.0592\n",
            "Epoch 237/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 16.2611 - mae: 0.5489 - val_loss: 0.0285 - val_mae: 0.0689\n",
            "Epoch 238/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0725 - mae: 0.2523 - val_loss: 0.0345 - val_mae: 0.1022\n",
            "Epoch 239/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.8314 - mae: 0.6626 - val_loss: 0.0367 - val_mae: 0.1123\n",
            "Epoch 240/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0898 - mae: 0.1619 - val_loss: 0.0312 - val_mae: 0.0843\n",
            "Epoch 241/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.5519 - mae: 0.5024 - val_loss: 0.0347 - val_mae: 0.1031\n",
            "Epoch 242/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8221 - mae: 0.3809 - val_loss: 0.0242 - val_mae: 0.0795\n",
            "Epoch 243/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.6461 - mae: 0.5450 - val_loss: 0.0241 - val_mae: 0.0820\n",
            "Epoch 244/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 9.5895 - mae: 0.5108 - val_loss: 0.0286 - val_mae: 0.0695\n",
            "Epoch 245/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5739 - mae: 0.2539 - val_loss: 0.0256 - val_mae: 0.0503\n",
            "Epoch 246/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3299 - mae: 0.4854 - val_loss: 0.0271 - val_mae: 0.0596\n",
            "Epoch 247/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3376 - mae: 0.3292 - val_loss: 0.0255 - val_mae: 0.0507\n",
            "Epoch 248/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6253 - mae: 0.2323 - val_loss: 0.0280 - val_mae: 0.0658\n",
            "Epoch 249/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3146 - mae: 0.2811 - val_loss: 0.0267 - val_mae: 0.0567\n",
            "Epoch 250/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3004 - mae: 0.2277 - val_loss: 0.0257 - val_mae: 0.0506\n",
            "Epoch 251/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 11.5470 - mae: 0.5612 - val_loss: 0.0241 - val_mae: 0.0773\n",
            "Epoch 252/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.9046 - mae: 0.6081 - val_loss: 0.0254 - val_mae: 0.0511\n",
            "Epoch 253/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 14.3315 - mae: 0.5855 - val_loss: 0.0270 - val_mae: 0.0580\n",
            "Epoch 254/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.4576 - mae: 0.4045 - val_loss: 0.0318 - val_mae: 0.0873\n",
            "Epoch 255/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.2714 - mae: 0.4777 - val_loss: 0.0242 - val_mae: 0.0757\n",
            "Epoch 256/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5336 - mae: 0.2620 - val_loss: 0.0259 - val_mae: 0.0504\n",
            "Epoch 257/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7220 - mae: 0.1991 - val_loss: 0.0271 - val_mae: 0.0598\n",
            "Epoch 258/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3285 - mae: 0.2916 - val_loss: 0.0243 - val_mae: 0.0686\n",
            "Epoch 259/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.3719 - mae: 0.4160 - val_loss: 0.0242 - val_mae: 0.0728\n",
            "Epoch 260/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45.9663 - mae: 0.9697 - val_loss: 0.0242 - val_mae: 0.0864\n",
            "Epoch 261/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6020 - mae: 0.2648 - val_loss: 0.0247 - val_mae: 0.0600\n",
            "Epoch 262/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0123 - mae: 0.2499 - val_loss: 0.0249 - val_mae: 0.0567\n",
            "Epoch 263/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.2946 - mae: 0.4372 - val_loss: 0.0254 - val_mae: 0.0509\n",
            "Epoch 264/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7396 - mae: 0.2570 - val_loss: 0.0243 - val_mae: 0.0715\n",
            "Epoch 265/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9160 - mae: 0.2956 - val_loss: 0.0254 - val_mae: 0.0515\n",
            "Epoch 266/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.8049 - mae: 0.3801 - val_loss: 0.0243 - val_mae: 0.0867\n",
            "Epoch 267/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3998 - mae: 0.4668 - val_loss: 0.0271 - val_mae: 0.0590\n",
            "Epoch 268/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.5285 - mae: 0.5891 - val_loss: 0.0242 - val_mae: 0.0869\n",
            "Epoch 269/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7339 - mae: 0.4539 - val_loss: 0.0245 - val_mae: 0.0628\n",
            "Epoch 270/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1211 - mae: 0.5937 - val_loss: 0.0278 - val_mae: 0.0645\n",
            "Epoch 271/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5485 - mae: 0.2942 - val_loss: 0.0407 - val_mae: 0.1291\n",
            "Epoch 272/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8121 - mae: 0.2624 - val_loss: 0.0277 - val_mae: 0.0632\n",
            "Epoch 273/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9162 - mae: 0.2980 - val_loss: 0.0260 - val_mae: 0.0502\n",
            "Epoch 274/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6061 - mae: 0.2003 - val_loss: 0.0269 - val_mae: 0.0575\n",
            "Epoch 275/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3468 - mae: 0.4196 - val_loss: 0.0309 - val_mae: 0.1433\n",
            "Epoch 276/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.9021 - mae: 0.4282 - val_loss: 0.0243 - val_mae: 0.0838\n",
            "Epoch 277/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4802 - mae: 0.2887 - val_loss: 0.0259 - val_mae: 0.0502\n",
            "Epoch 278/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3834 - mae: 0.4367 - val_loss: 0.0249 - val_mae: 0.0577\n",
            "Epoch 279/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1195 - mae: 0.1070 - val_loss: 0.0244 - val_mae: 0.0676\n",
            "Epoch 280/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 45.9977 - mae: 1.1196 - val_loss: 0.0248 - val_mae: 0.0594\n",
            "Epoch 281/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 7.1682 - mae: 0.4061 - val_loss: 0.0249 - val_mae: 0.0589\n",
            "Epoch 282/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5558 - mae: 0.1989 - val_loss: 0.0245 - val_mae: 0.0638\n",
            "Epoch 283/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4645 - mae: 0.2677 - val_loss: 0.0244 - val_mae: 0.0661\n",
            "Epoch 284/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0063 - mae: 0.3678 - val_loss: 0.0384 - val_mae: 0.1761\n",
            "Epoch 285/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.2173 - mae: 0.4421 - val_loss: 0.0270 - val_mae: 0.0597\n",
            "Epoch 286/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.7151 - mae: 0.5278 - val_loss: 0.0282 - val_mae: 0.0671\n",
            "Epoch 287/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.5232 - mae: 0.3585 - val_loss: 0.0256 - val_mae: 0.0500\n",
            "Epoch 288/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7066 - mae: 0.2325 - val_loss: 0.0258 - val_mae: 0.0499\n",
            "Epoch 289/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4650 - mae: 0.3209 - val_loss: 0.0348 - val_mae: 0.1033\n",
            "Epoch 290/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5534 - mae: 0.2349 - val_loss: 0.0242 - val_mae: 0.0854\n",
            "Epoch 291/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0370 - mae: 0.2403 - val_loss: 0.0247 - val_mae: 0.0601\n",
            "Epoch 292/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5501 - mae: 0.3288 - val_loss: 0.0344 - val_mae: 0.1015\n",
            "Epoch 293/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5477 - mae: 0.5617 - val_loss: 0.0242 - val_mae: 0.0860\n",
            "Epoch 294/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2419 - mae: 0.4779 - val_loss: 0.0271 - val_mae: 0.0595\n",
            "Epoch 295/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0127 - mae: 0.1751 - val_loss: 0.0247 - val_mae: 0.0625\n",
            "Epoch 296/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4673 - mae: 0.1739 - val_loss: 0.0257 - val_mae: 0.0498\n",
            "Epoch 297/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3770 - mae: 0.1449 - val_loss: 0.0244 - val_mae: 0.0663\n",
            "Epoch 298/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3864 - mae: 0.1746 - val_loss: 0.0245 - val_mae: 0.0644\n",
            "Epoch 299/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0412 - mae: 0.2944 - val_loss: 0.0243 - val_mae: 0.0894\n",
            "Epoch 300/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 10.1351 - mae: 0.5362 - val_loss: 0.0259 - val_mae: 0.0504\n",
            "Epoch 301/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9587 - mae: 0.2471 - val_loss: 0.0256 - val_mae: 0.1080\n",
            "Epoch 302/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.5937 - mae: 0.4164 - val_loss: 0.0243 - val_mae: 0.0681\n",
            "Epoch 303/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3520 - mae: 0.1702 - val_loss: 0.0244 - val_mae: 0.0669\n",
            "Epoch 304/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3976 - mae: 0.3110 - val_loss: 0.0306 - val_mae: 0.0811\n",
            "Epoch 305/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1679 - mae: 0.1583 - val_loss: 0.0247 - val_mae: 0.0626\n",
            "Epoch 306/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4016 - mae: 0.1653 - val_loss: 0.0243 - val_mae: 0.0766\n",
            "Epoch 307/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9492 - mae: 0.3776 - val_loss: 0.0335 - val_mae: 0.0977\n",
            "Epoch 308/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.2249 - mae: 0.6611 - val_loss: 0.0513 - val_mae: 0.2152\n",
            "Epoch 309/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.9831 - mae: 0.4798 - val_loss: 0.0246 - val_mae: 0.0892\n",
            "Epoch 310/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6312 - mae: 0.2293 - val_loss: 0.0250 - val_mae: 0.1012\n",
            "Epoch 311/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9282 - mae: 0.4709 - val_loss: 0.0242 - val_mae: 0.0753\n",
            "Epoch 312/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.2105 - mae: 0.3274 - val_loss: 0.0242 - val_mae: 0.0815\n",
            "Epoch 313/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1846 - mae: 0.3563 - val_loss: 0.0249 - val_mae: 0.0722\n",
            "Epoch 314/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 12.1775 - mae: 0.6201 - val_loss: 0.0245 - val_mae: 0.0657\n",
            "Epoch 315/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5130 - mae: 0.1756 - val_loss: 0.0251 - val_mae: 0.0556\n",
            "Epoch 316/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9173 - mae: 0.3281 - val_loss: 0.0261 - val_mae: 0.0511\n",
            "Epoch 317/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3647 - mae: 0.1918 - val_loss: 0.0298 - val_mae: 0.0767\n",
            "Epoch 318/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1333 - mae: 0.1829 - val_loss: 0.0265 - val_mae: 0.0533\n",
            "Epoch 319/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9334 - mae: 0.2921 - val_loss: 0.0244 - val_mae: 0.0918\n",
            "Epoch 320/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5422 - mae: 0.2840 - val_loss: 0.0251 - val_mae: 0.0560\n",
            "Epoch 321/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4015 - mae: 0.2078 - val_loss: 0.0249 - val_mae: 0.0584\n",
            "Epoch 322/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6087 - mae: 0.2892 - val_loss: 0.0243 - val_mae: 0.0736\n",
            "Epoch 323/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.5036 - mae: 0.3565 - val_loss: 0.0242 - val_mae: 0.0750\n",
            "Epoch 324/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6353 - mae: 0.3348 - val_loss: 0.0549 - val_mae: 0.2237\n",
            "Epoch 325/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1132 - mae: 0.4822 - val_loss: 0.0298 - val_mae: 0.1381\n",
            "Epoch 326/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1108 - mae: 0.2744 - val_loss: 0.0378 - val_mae: 0.1165\n",
            "Epoch 327/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.3281 - mae: 0.2887 - val_loss: 0.0267 - val_mae: 0.0549\n",
            "Epoch 328/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.2600 - mae: 0.3878 - val_loss: 0.0243 - val_mae: 0.0889\n",
            "Epoch 329/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2082 - mae: 0.1819 - val_loss: 0.0250 - val_mae: 0.0563\n",
            "Epoch 330/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8381 - mae: 0.2665 - val_loss: 0.0262 - val_mae: 0.0526\n",
            "Epoch 331/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8117 - mae: 0.2060 - val_loss: 0.0263 - val_mae: 0.0534\n",
            "Epoch 332/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3141 - mae: 0.2546 - val_loss: 0.0260 - val_mae: 0.0504\n",
            "Epoch 333/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 9.4114 - mae: 0.3238 - val_loss: 0.0242 - val_mae: 0.0784\n",
            "Epoch 334/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2811 - mae: 0.1788 - val_loss: 0.0361 - val_mae: 0.1093\n",
            "Epoch 335/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2113 - mae: 0.2371 - val_loss: 0.0316 - val_mae: 0.0863\n",
            "Epoch 336/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4423 - mae: 0.2265 - val_loss: 0.0277 - val_mae: 0.0640\n",
            "Epoch 337/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4510 - mae: 0.2232 - val_loss: 0.0249 - val_mae: 0.0573\n",
            "Epoch 338/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3005 - mae: 0.2093 - val_loss: 0.0254 - val_mae: 0.0510\n",
            "Epoch 339/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.1258 - mae: 0.2292 - val_loss: 0.0257 - val_mae: 0.1106\n",
            "Epoch 340/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6506 - mae: 0.2515 - val_loss: 0.0414 - val_mae: 0.1316\n",
            "Epoch 341/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0583 - mae: 0.3765 - val_loss: 0.0313 - val_mae: 0.0856\n",
            "Epoch 342/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0148 - mae: 0.3571 - val_loss: 0.0303 - val_mae: 0.0796\n",
            "Epoch 343/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4864 - mae: 0.3326 - val_loss: 0.0249 - val_mae: 0.0577\n",
            "Epoch 344/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1283 - mae: 0.1078 - val_loss: 0.0249 - val_mae: 0.1002\n",
            "Epoch 345/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 5.8043 - mae: 0.4436 - val_loss: 0.0243 - val_mae: 0.0868\n",
            "Epoch 346/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.5308 - mae: 0.1974 - val_loss: 0.0261 - val_mae: 0.0508\n",
            "Epoch 347/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8557 - mae: 0.2760 - val_loss: 0.0246 - val_mae: 0.0616\n",
            "Epoch 348/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.3595 - mae: 0.2237 - val_loss: 0.0242 - val_mae: 0.0777\n",
            "Epoch 349/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0353 - mae: 0.2192 - val_loss: 0.0274 - val_mae: 0.0623\n",
            "Epoch 350/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1374 - mae: 0.1410 - val_loss: 0.0258 - val_mae: 0.0497\n",
            "Epoch 351/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1533 - mae: 0.1103 - val_loss: 0.0249 - val_mae: 0.1000\n",
            "Epoch 352/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7138 - mae: 0.2463 - val_loss: 0.0254 - val_mae: 0.0512\n",
            "Epoch 353/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 6.4932 - mae: 0.4377 - val_loss: 0.0251 - val_mae: 0.0558\n",
            "Epoch 354/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5659 - mae: 0.1814 - val_loss: 0.0248 - val_mae: 0.0595\n",
            "Epoch 355/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4760 - mae: 0.2572 - val_loss: 0.0262 - val_mae: 0.0531\n",
            "Epoch 356/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7596 - mae: 0.2544 - val_loss: 0.0242 - val_mae: 0.0769\n",
            "Epoch 357/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4162 - mae: 0.1940 - val_loss: 0.0250 - val_mae: 0.0560\n",
            "Epoch 358/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6136 - mae: 0.1852 - val_loss: 0.0242 - val_mae: 0.0752\n",
            "Epoch 359/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5682 - mae: 0.2679 - val_loss: 0.0248 - val_mae: 0.0995\n",
            "Epoch 360/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0863 - mae: 0.1372 - val_loss: 0.0251 - val_mae: 0.0550\n",
            "Epoch 361/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5513 - mae: 0.1647 - val_loss: 0.0255 - val_mae: 0.1073\n",
            "Epoch 362/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7084 - mae: 0.1960 - val_loss: 0.0260 - val_mae: 0.0510\n",
            "Epoch 363/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4245 - mae: 0.1734 - val_loss: 0.0253 - val_mae: 0.1051\n",
            "Epoch 364/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.4859 - mae: 0.2554 - val_loss: 0.0248 - val_mae: 0.0583\n",
            "Epoch 365/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2919 - mae: 0.1612 - val_loss: 0.0246 - val_mae: 0.0628\n",
            "Epoch 366/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3476 - mae: 0.2670 - val_loss: 0.0245 - val_mae: 0.0943\n",
            "Epoch 367/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8799 - mae: 0.2699 - val_loss: 0.0241 - val_mae: 0.0754\n",
            "Epoch 368/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5870 - mae: 0.1920 - val_loss: 0.0260 - val_mae: 0.0503\n",
            "Epoch 369/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0526 - mae: 0.0856 - val_loss: 0.0241 - val_mae: 0.0787\n",
            "Epoch 370/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.2770 - mae: 0.2327 - val_loss: 0.0263 - val_mae: 0.0534\n",
            "Epoch 371/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9119 - mae: 0.1930 - val_loss: 0.0250 - val_mae: 0.1005\n",
            "Epoch 372/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7160 - mae: 0.3099 - val_loss: 0.0258 - val_mae: 0.0498\n",
            "Epoch 373/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2143 - mae: 0.1747 - val_loss: 0.0243 - val_mae: 0.0676\n",
            "Epoch 374/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.0967 - mae: 0.1750 - val_loss: 0.0245 - val_mae: 0.0640\n",
            "Epoch 375/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.9765 - mae: 0.2879 - val_loss: 0.0245 - val_mae: 0.0641\n",
            "Epoch 376/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0981 - mae: 0.1194 - val_loss: 0.0247 - val_mae: 0.0959\n",
            "Epoch 377/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1025 - mae: 0.1440 - val_loss: 0.0252 - val_mae: 0.1036\n",
            "Epoch 378/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.4682 - mae: 0.4791 - val_loss: 0.0305 - val_mae: 0.0814\n",
            "Epoch 379/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6714 - mae: 0.1964 - val_loss: 0.0243 - val_mae: 0.0679\n",
            "Epoch 380/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4741 - mae: 0.1367 - val_loss: 0.0256 - val_mae: 0.0499\n",
            "Epoch 381/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2758 - mae: 0.1348 - val_loss: 0.0255 - val_mae: 0.0505\n",
            "Epoch 382/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5542 - mae: 0.1637 - val_loss: 0.0242 - val_mae: 0.0764\n",
            "Epoch 383/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3842 - mae: 0.1862 - val_loss: 0.0242 - val_mae: 0.0810\n",
            "Epoch 384/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4033 - mae: 0.2109 - val_loss: 0.0299 - val_mae: 0.0775\n",
            "Epoch 385/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4764 - mae: 0.1819 - val_loss: 0.0284 - val_mae: 0.1295\n",
            "Epoch 386/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2702 - mae: 0.1839 - val_loss: 0.0244 - val_mae: 0.0893\n",
            "Epoch 387/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4096 - mae: 0.1726 - val_loss: 0.0248 - val_mae: 0.0592\n",
            "Epoch 388/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7881 - mae: 0.1927 - val_loss: 0.0258 - val_mae: 0.0498\n",
            "Epoch 389/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0878 - mae: 0.1169 - val_loss: 0.0245 - val_mae: 0.0931\n",
            "Epoch 390/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4911 - mae: 0.1728 - val_loss: 0.0246 - val_mae: 0.0610\n",
            "Epoch 391/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.7190 - mae: 0.3358 - val_loss: 0.0241 - val_mae: 0.0821\n",
            "Epoch 392/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1396 - mae: 0.3235 - val_loss: 0.0241 - val_mae: 0.0793\n",
            "Epoch 393/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1269 - mae: 0.1168 - val_loss: 0.0247 - val_mae: 0.0975\n",
            "Epoch 394/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4272 - mae: 0.1737 - val_loss: 0.0246 - val_mae: 0.0962\n",
            "Epoch 395/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3286 - mae: 0.1727 - val_loss: 0.0271 - val_mae: 0.0600\n",
            "Epoch 396/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.8157 - mae: 0.2718 - val_loss: 0.0244 - val_mae: 0.0655\n",
            "Epoch 397/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9593 - mae: 0.2952 - val_loss: 0.0343 - val_mae: 0.1015\n",
            "Epoch 398/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3208 - mae: 0.3034 - val_loss: 0.0249 - val_mae: 0.0565\n",
            "Epoch 399/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6497 - mae: 0.2172 - val_loss: 0.0257 - val_mae: 0.0497\n",
            "Epoch 400/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1925 - mae: 0.2202 - val_loss: 0.0250 - val_mae: 0.0553\n",
            "Epoch 401/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6416 - mae: 0.2952 - val_loss: 0.0245 - val_mae: 0.0946\n",
            "Epoch 402/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3856 - mae: 0.1965 - val_loss: 0.0259 - val_mae: 0.1125\n",
            "Epoch 403/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3878 - mae: 0.1533 - val_loss: 0.0245 - val_mae: 0.0632\n",
            "Epoch 404/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7688 - mae: 0.2564 - val_loss: 0.0268 - val_mae: 0.0582\n",
            "Epoch 405/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5888 - mae: 0.1774 - val_loss: 0.0245 - val_mae: 0.0946\n",
            "Epoch 406/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6077 - mae: 0.1625 - val_loss: 0.0397 - val_mae: 0.1803\n",
            "Epoch 407/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.8794 - mae: 0.3026 - val_loss: 0.0285 - val_mae: 0.1303\n",
            "Epoch 408/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5881 - mae: 0.2476 - val_loss: 0.0242 - val_mae: 0.0709\n",
            "Epoch 409/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1306 - mae: 0.1272 - val_loss: 0.0242 - val_mae: 0.0717\n",
            "Epoch 410/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1399 - mae: 0.1266 - val_loss: 0.0249 - val_mae: 0.1010\n",
            "Epoch 411/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0412 - mae: 0.1215 - val_loss: 0.0243 - val_mae: 0.0878\n",
            "Epoch 412/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0995 - mae: 0.1570 - val_loss: 0.0330 - val_mae: 0.0947\n",
            "Epoch 413/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.1591 - mae: 0.2280 - val_loss: 0.0242 - val_mae: 0.0855\n",
            "Epoch 414/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0482 - mae: 0.1151 - val_loss: 0.0258 - val_mae: 0.0499\n",
            "Epoch 415/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.7084 - mae: 0.2352 - val_loss: 0.0258 - val_mae: 0.1104\n",
            "Epoch 416/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0945 - mae: 0.1479 - val_loss: 0.0243 - val_mae: 0.0668\n",
            "Epoch 417/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0560 - mae: 0.0992 - val_loss: 0.0361 - val_mae: 0.1674\n",
            "Epoch 418/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.3598 - mae: 0.2427 - val_loss: 0.0245 - val_mae: 0.0941\n",
            "Epoch 419/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6704 - mae: 0.2931 - val_loss: 0.0251 - val_mae: 0.0532\n",
            "Epoch 420/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9177 - mae: 0.2063 - val_loss: 0.0246 - val_mae: 0.0598\n",
            "Epoch 421/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1087 - mae: 0.1659 - val_loss: 0.0273 - val_mae: 0.0610\n",
            "Epoch 422/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3875 - mae: 0.1684 - val_loss: 0.0280 - val_mae: 0.1274\n",
            "Epoch 423/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2459 - mae: 0.2359 - val_loss: 0.0313 - val_mae: 0.0855\n",
            "Epoch 424/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2257 - mae: 0.1751 - val_loss: 0.0242 - val_mae: 0.0837\n",
            "Epoch 425/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.8862 - mae: 0.2441 - val_loss: 0.0247 - val_mae: 0.0600\n",
            "Epoch 426/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5750 - mae: 0.1615 - val_loss: 0.0241 - val_mae: 0.0762\n",
            "Epoch 427/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1272 - mae: 0.1982 - val_loss: 0.0285 - val_mae: 0.1302\n",
            "Epoch 428/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0852 - mae: 0.1656 - val_loss: 0.0243 - val_mae: 0.0903\n",
            "Epoch 429/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6824 - mae: 0.2290 - val_loss: 0.0244 - val_mae: 0.0920\n",
            "Epoch 430/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0567 - mae: 0.1207 - val_loss: 0.0242 - val_mae: 0.0868\n",
            "Epoch 431/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2438 - mae: 0.1578 - val_loss: 0.0280 - val_mae: 0.1273\n",
            "Epoch 432/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1338 - mae: 0.1429 - val_loss: 0.0241 - val_mae: 0.0833\n",
            "Epoch 433/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4547 - mae: 0.1321 - val_loss: 0.0245 - val_mae: 0.0960\n",
            "Epoch 434/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1388 - mae: 0.1556 - val_loss: 0.0242 - val_mae: 0.0837\n",
            "Epoch 435/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2420 - mae: 0.1635 - val_loss: 0.0242 - val_mae: 0.0873\n",
            "Epoch 436/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.9387 - mae: 0.2563 - val_loss: 0.0241 - val_mae: 0.0757\n",
            "Epoch 437/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4469 - mae: 0.1824 - val_loss: 0.0249 - val_mae: 0.0564\n",
            "Epoch 438/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 4.1603 - mae: 0.2526 - val_loss: 0.0243 - val_mae: 0.0897\n",
            "Epoch 439/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6635 - mae: 0.1753 - val_loss: 0.0246 - val_mae: 0.0610\n",
            "Epoch 440/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2072 - mae: 0.1837 - val_loss: 0.0241 - val_mae: 0.0778\n",
            "Epoch 441/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1174 - mae: 0.1473 - val_loss: 0.0243 - val_mae: 0.0672\n",
            "Epoch 442/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0698 - mae: 0.1046 - val_loss: 0.0241 - val_mae: 0.0750\n",
            "Epoch 443/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.1485 - mae: 0.2070 - val_loss: 0.0242 - val_mae: 0.0741\n",
            "Epoch 444/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0524 - mae: 0.1094 - val_loss: 0.0241 - val_mae: 0.0792\n",
            "Epoch 445/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0847 - mae: 0.1373 - val_loss: 0.0339 - val_mae: 0.1581\n",
            "Epoch 446/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2097 - mae: 0.2063 - val_loss: 0.0263 - val_mae: 0.1147\n",
            "Epoch 447/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0992 - mae: 0.1847 - val_loss: 0.0246 - val_mae: 0.0618\n",
            "Epoch 448/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1749 - mae: 0.1306 - val_loss: 0.0265 - val_mae: 0.0552\n",
            "Epoch 449/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7519 - mae: 0.1948 - val_loss: 0.0248 - val_mae: 0.0988\n",
            "Epoch 450/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0730 - mae: 0.1507 - val_loss: 0.0245 - val_mae: 0.0937\n",
            "Epoch 451/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0737 - mae: 0.1602 - val_loss: 0.0392 - val_mae: 0.1235\n",
            "Epoch 452/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2903 - mae: 0.2240 - val_loss: 0.0245 - val_mae: 0.0639\n",
            "Epoch 453/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3794 - mae: 0.1608 - val_loss: 0.0243 - val_mae: 0.0679\n",
            "Epoch 454/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4973 - mae: 0.2247 - val_loss: 0.0243 - val_mae: 0.0676\n",
            "Epoch 455/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0743 - mae: 0.1361 - val_loss: 0.0368 - val_mae: 0.1133\n",
            "Epoch 456/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2349 - mae: 0.2359 - val_loss: 0.0270 - val_mae: 0.1208\n",
            "Epoch 457/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6013 - mae: 0.1954 - val_loss: 0.0248 - val_mae: 0.0990\n",
            "Epoch 458/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2928 - mae: 0.2484 - val_loss: 0.0361 - val_mae: 0.1672\n",
            "Epoch 459/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2852 - mae: 0.2296 - val_loss: 0.1195 - val_mae: 0.3427\n",
            "Epoch 460/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2581 - mae: 0.2735 - val_loss: 0.0241 - val_mae: 0.0751\n",
            "Epoch 461/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0479 - mae: 0.1048 - val_loss: 0.0245 - val_mae: 0.0632\n",
            "Epoch 462/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1069 - mae: 0.1390 - val_loss: 0.0255 - val_mae: 0.0501\n",
            "Epoch 463/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2076 - mae: 0.1642 - val_loss: 0.0269 - val_mae: 0.1205\n",
            "Epoch 464/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5532 - mae: 0.1861 - val_loss: 0.0245 - val_mae: 0.0941\n",
            "Epoch 465/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0822 - mae: 0.1951 - val_loss: 0.0245 - val_mae: 0.0621\n",
            "Epoch 466/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2268 - mae: 0.1789 - val_loss: 0.0241 - val_mae: 0.0778\n",
            "Epoch 467/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0609 - mae: 0.1244 - val_loss: 0.0350 - val_mae: 0.1627\n",
            "Epoch 468/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1423 - mae: 0.1624 - val_loss: 0.0252 - val_mae: 0.1045\n",
            "Epoch 469/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0559 - mae: 0.1232 - val_loss: 0.0250 - val_mae: 0.1030\n",
            "Epoch 470/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.3181 - mae: 0.1789 - val_loss: 0.0280 - val_mae: 0.0660\n",
            "Epoch 471/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0542 - mae: 0.1253 - val_loss: 0.0253 - val_mae: 0.0517\n",
            "Epoch 472/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.8392 - mae: 0.3421 - val_loss: 0.0346 - val_mae: 0.1032\n",
            "Epoch 473/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1504 - mae: 0.1787 - val_loss: 0.0350 - val_mae: 0.1628\n",
            "Epoch 474/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0945 - mae: 0.1543 - val_loss: 0.0249 - val_mae: 0.1005\n",
            "Epoch 475/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7486 - mae: 0.1722 - val_loss: 0.0512 - val_mae: 0.2152\n",
            "Epoch 476/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.5117 - mae: 0.2652 - val_loss: 0.0272 - val_mae: 0.0606\n",
            "Epoch 477/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1508 - mae: 0.1846 - val_loss: 0.0267 - val_mae: 0.0570\n",
            "Epoch 478/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0680 - mae: 0.1242 - val_loss: 0.0272 - val_mae: 0.1229\n",
            "Epoch 479/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2593 - mae: 0.2017 - val_loss: 0.0433 - val_mae: 0.1926\n",
            "Epoch 480/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0936 - mae: 0.2009 - val_loss: 0.0347 - val_mae: 0.1614\n",
            "Epoch 481/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1131 - mae: 0.1743 - val_loss: 0.0242 - val_mae: 0.0859\n",
            "Epoch 482/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1522 - mae: 0.1475 - val_loss: 0.0390 - val_mae: 0.1781\n",
            "Epoch 483/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1072 - mae: 0.1929 - val_loss: 0.0253 - val_mae: 0.1051\n",
            "Epoch 484/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1552 - mae: 0.1586 - val_loss: 0.0244 - val_mae: 0.0648\n",
            "Epoch 485/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2115 - mae: 0.1516 - val_loss: 0.0242 - val_mae: 0.0852\n",
            "Epoch 486/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0991 - mae: 0.1919 - val_loss: 0.0247 - val_mae: 0.0586\n",
            "Epoch 487/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4925 - mae: 0.1859 - val_loss: 0.0260 - val_mae: 0.1131\n",
            "Epoch 488/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1561 - mae: 0.2152 - val_loss: 0.0438 - val_mae: 0.1410\n",
            "Epoch 489/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1683 - mae: 0.3322 - val_loss: 0.0372 - val_mae: 0.1714\n",
            "Epoch 490/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1378 - mae: 0.1814 - val_loss: 0.0244 - val_mae: 0.0648\n",
            "Epoch 491/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1957 - mae: 0.1920 - val_loss: 0.0242 - val_mae: 0.0817\n",
            "Epoch 492/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1979 - mae: 0.1823 - val_loss: 0.0250 - val_mae: 0.0562\n",
            "Epoch 493/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1264 - mae: 0.1637 - val_loss: 0.0626 - val_mae: 0.2425\n",
            "Epoch 494/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1280 - mae: 0.2736 - val_loss: 0.0325 - val_mae: 0.0923\n",
            "Epoch 495/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0731 - mae: 0.1740 - val_loss: 0.0274 - val_mae: 0.1241\n",
            "Epoch 496/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1069 - mae: 0.1495 - val_loss: 0.0378 - val_mae: 0.1735\n",
            "Epoch 497/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.1096 - mae: 0.2137 - val_loss: 0.0279 - val_mae: 0.0656\n",
            "Epoch 498/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0748 - mae: 0.1763 - val_loss: 0.0286 - val_mae: 0.0703\n",
            "Epoch 499/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0482 - mae: 0.1209 - val_loss: 0.0251 - val_mae: 0.0535\n",
            "Epoch 500/500\n",
            "\u001b[1m23/23\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.6342 - mae: 0.2444 - val_loss: 0.0297 - val_mae: 0.0770\n",
            "LSTM Regression Model Test Loss: 0.023398470133543015, Test MAE: 0.06895077973604202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, num_labels, nhead=4, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
        "        super(SimpleTransformer, self).__init__()\n",
        "        # Transformer Encoder Layer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=input_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        # Fully Connected Layer for Output\n",
        "        self.fc = nn.Linear(input_dim, num_labels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Transformer expects input shape: (seq_length, batch_size, input_dim)\n",
        "        x = x.permute(1, 0, 2)  # Change to (seq_length, batch_size, input_dim)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=0)  # Pooling (mean over sequence)\n",
        "        x = self.fc(x)  # Output layer\n",
        "        return torch.sigmoid(x)  # Apply sigmoid for multi-label classification\n",
        "\n",
        "# Model Hyperparameters\n",
        "input_dim = X_train_seq.shape[2]  # Number of features\n",
        "num_labels = y_morphology_train_seq.shape[1]  # Number of output labels\n",
        "nhead = 4\n",
        "num_layers = 2\n",
        "dim_feedforward = 128\n",
        "dropout = 0.1\n",
        "\n",
        "# Initialize the model\n",
        "model = SimpleTransformer(input_dim, num_labels, nhead, num_layers, dim_feedforward, dropout)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "loss_fn = nn.BCELoss()  # Binary Cross Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Data Preparation\n",
        "train_loader = DataLoader(list(zip(X_train_seq, y_morphology_train_seq)), batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(list(zip(X_val_seq, y_morphology_val_seq)), batch_size=16, shuffle=False)\n",
        "test_loader = DataLoader(list(zip(X_test_seq, y_morphology_test_seq)), batch_size=16, shuffle=False)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in train_loader:\n",
        "        X_batch, y_batch = batch\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        y_batch = y_batch.to(device).float()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_batch)\n",
        "        loss = loss_fn(outputs, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluation Loop\n",
        "model.eval()\n",
        "test_labels = []\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        X_batch, y_batch = batch\n",
        "        X_batch = X_batch.to(device).float()\n",
        "        outputs = model(X_batch)\n",
        "        test_preds.append(outputs.cpu().numpy())\n",
        "        test_labels.append(y_batch.numpy())\n",
        "\n",
        "# Combine predictions and true labels\n",
        "test_labels = np.vstack(test_labels)\n",
        "test_preds = np.vstack(test_preds)\n",
        "\n",
        "# Threshold predictions\n",
        "threshold = 0.5\n",
        "test_preds_binary = (test_preds > threshold).astype(int)\n",
        "\n",
        "# Calculate Evaluation Metrics\n",
        "accuracy = accuracy_score(test_labels.flatten(), test_preds_binary.flatten())\n",
        "f1 = f1_score(test_labels, test_preds_binary, average='micro')\n",
        "roc_auc = roc_auc_score(test_labels, test_preds, average='micro')\n",
        "\n",
        "print(\"Test Evaluation:\")\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"F1-Score (Micro): {f1}\")\n",
        "print(f\"ROC-AUC (Micro): {roc_auc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztFgu74oUjZO",
        "outputId": "e3bdab2a-6463-4590-95c8-2b09bdf39714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5453366326249164\n",
            "Epoch 2, Loss: 0.3825240718281787\n",
            "Epoch 3, Loss: 0.3576572589252306\n",
            "Epoch 4, Loss: 0.35416153721187427\n",
            "Epoch 5, Loss: 0.3524011878863625\n",
            "Epoch 6, Loss: 0.34911297456077905\n",
            "Epoch 7, Loss: 0.34688816899838654\n",
            "Epoch 8, Loss: 0.3473867411198823\n",
            "Epoch 9, Loss: 0.3412993848323822\n",
            "Epoch 10, Loss: 0.33860402133153833\n",
            "Test Evaluation:\n",
            "Accuracy: 0.8547368421052631\n",
            "F1-Score (Micro): 0.14107883817427386\n",
            "ROC-AUC (Micro): 0.7636265493802479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Njp_qnPFUoxJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"simple_transformer_model.pth\")\n",
        "joblib.dump(scaler, \"scaler.pkl\")  # Save the scaler if used\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0maTfHf3XPzE",
        "outputId": "3c62488a-a461-4bd6-ac05-5fee41747033"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "def simulate_polyp_evolution_with_debug(initial_features, size_model, morphology_model, time_steps=5):\n",
        "    \"\"\"\n",
        "    Simulate the evolution of polyp characteristics over time with debug logs.\n",
        "\n",
        "    Args:\n",
        "        initial_features (np.array): Initial feature vector for the polyp.\n",
        "        size_model: Trained TensorFlow/Keras regression model for size prediction.\n",
        "        morphology_model: Trained PyTorch classification model for morphological evolution.\n",
        "        time_steps (int): Number of future time steps to simulate.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Simulated polyp evolution over time.\n",
        "    \"\"\"\n",
        "    morphology_feature_count = morphology_model.fc.out_features  # Number of morphology features\n",
        "    results = []\n",
        "    current_features = initial_features.copy()\n",
        "\n",
        "    for t in range(1, time_steps + 1):\n",
        "        # Predict size\n",
        "        reshaped_features_tf = current_features.reshape(1, 1, -1)  # Add batch and timestep dimensions\n",
        "        predicted_size = size_model.predict(reshaped_features_tf)[0, 0]\n",
        "\n",
        "        # Log current state\n",
        "        print(f\"Time Step {t}: Predicted Size = {predicted_size}\")\n",
        "\n",
        "        # Update size in the feature vector\n",
        "        current_features[0] = predicted_size\n",
        "\n",
        "        # Predict morphology\n",
        "        reshaped_features_torch = torch.tensor(current_features, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            morphology_probs = morphology_model(reshaped_features_torch).numpy()\n",
        "        morphology_classes = (morphology_probs > 0.5).astype(int)\n",
        "\n",
        "        # Update morphology features\n",
        "        morphology_start_index = 1\n",
        "        morphology_end_index = morphology_start_index + morphology_feature_count\n",
        "        current_features[morphology_start_index:morphology_end_index] = morphology_classes.flatten()\n",
        "\n",
        "        # Save results\n",
        "        results.append({\n",
        "            \"Time_Step\": t,\n",
        "            \"Predicted_Size\": predicted_size,\n",
        "            \"Morphology_Classes\": morphology_classes.flatten().tolist()\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage\n",
        "# Select a sample polyp's initial features\n",
        "sample_polyp_features = X_test_seq[0, -1, :]  # Last timestep features for a polyp\n",
        "\n",
        "# Simulate evolution\n",
        "predicted_evolution = simulate_polyp_evolution(\n",
        "    initial_features=sample_polyp_features,\n",
        "    size_model = lstm_model,  # Replace with the actual size regression model\n",
        "    morphology_model = model,  # Replace with the actual morphology model\n",
        "    time_steps=5\n",
        ")\n",
        "\n",
        "# Display results\n",
        "print(\"Predicted Polyp Evolution Over Time:\")\n",
        "print(predicted_evolution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqWPWJS4XbiU",
        "outputId": "4dba856c-d914-4b1d-9f4e-cdd2a7b313e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Predicted Polyp Evolution Over Time:\n",
            "   Time_Step  Predicted_Size  \\\n",
            "0          1        0.088947   \n",
            "1          2        0.099268   \n",
            "2          3        0.099270   \n",
            "3          4        0.099270   \n",
            "4          5        0.099270   \n",
            "\n",
            "                                  Morphology_Classes  \n",
            "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
            "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Morphology labels based on the dataset\n",
        "morphology_labels = [\n",
        "    \"Paris_0-IIa\", \"Paris_0-IIa + IIc\", \"Paris_0-IIa + Is\", \"Paris_0-IIa /c\",\n",
        "    \"Paris_0-IIb\", \"Paris_0-Ip\", \"Paris_0-Ips\", \"Paris_0-Is\", \"Paris_0-lps\", \"Paris_0-ls\",\n",
        "    \"Diagnosis_Adenocarcinoma\", \"Diagnosis_Hyperplastic\", \"Diagnosis_Inflammatory\",\n",
        "    \"Diagnosis_Serrated\", \"Diagnosis_Serrated, Hyperplastic\", \"Diagnosis_T + V\",\n",
        "    \"Diagnosis_Traditional Serrated Adenoma\", \"Diagnosis_Tubular\", \"Diagnosis_Villous\"\n",
        "]\n",
        "\n",
        "def decode_morphology_classes(morphology_classes, morphology_labels):\n",
        "    \"\"\"\n",
        "    Decode morphology classes into clinical terms.\n",
        "\n",
        "    Args:\n",
        "        morphology_classes (list): Binary list (0s and 1s) indicating active classes.\n",
        "        morphology_labels (list): List of clinical terms for morphology classes.\n",
        "\n",
        "    Returns:\n",
        "        str: Decoded clinical description.\n",
        "    \"\"\"\n",
        "    # Identify active classes\n",
        "    active_indices = [i for i, value in enumerate(morphology_classes) if value == 1]\n",
        "    active_terms = [morphology_labels[i] for i in active_indices]\n",
        "\n",
        "    # Create a descriptive statement\n",
        "    if active_terms:\n",
        "        return \", \".join(active_terms)\n",
        "    else:\n",
        "        return \"no significant morphological changes\"\n",
        "\n",
        "def generate_clinical_statements(predicted_evolution, morphology_labels):\n",
        "    \"\"\"\n",
        "    Generate clinical statements from simulation results.\n",
        "\n",
        "    Args:\n",
        "        predicted_evolution (pd.DataFrame): DataFrame with simulation results.\n",
        "        morphology_labels (list): List of clinical terms for morphology classes.\n",
        "\n",
        "    Returns:\n",
        "        list: List of clinical statements for each time step.\n",
        "    \"\"\"\n",
        "    statements = []\n",
        "    for _, row in predicted_evolution.iterrows():\n",
        "        time_step = row[\"Time_Step\"]\n",
        "        size = row[\"Predicted_Size\"]\n",
        "        morphology_classes = row[\"Morphology_Classes\"]\n",
        "\n",
        "        # Decode morphology classes\n",
        "        morphology_desc = decode_morphology_classes(morphology_classes, morphology_labels)\n",
        "\n",
        "        # Generate clinical statement\n",
        "        statement = (\n",
        "            f\"At time step {time_step}, the polyp size is {size:.2f} cm with morphology: {morphology_desc}.\"\n",
        "        )\n",
        "        statements.append(statement)\n",
        "\n",
        "    return statements\n",
        "\n",
        "# Example usage\n",
        "# Assuming `predicted_evolution` is the DataFrame from the simulation\n",
        "clinical_statements = generate_clinical_statements(predicted_evolution, morphology_labels)\n",
        "\n",
        "# Print the clinical statements\n",
        "print(\"Clinical Statements:\")\n",
        "for statement in clinical_statements:\n",
        "    print(statement)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZOc34iikYipu",
        "outputId": "67f7544b-cc61-4ec5-d499-bbe6553c523b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clinical Statements:\n",
            "At time step 1, the polyp size is 0.09 cm with morphology: Diagnosis_Tubular.\n",
            "At time step 2, the polyp size is 0.10 cm with morphology: Diagnosis_Tubular.\n",
            "At time step 3, the polyp size is 0.10 cm with morphology: Diagnosis_Tubular.\n",
            "At time step 4, the polyp size is 0.10 cm with morphology: Diagnosis_Tubular.\n",
            "At time step 5, the polyp size is 0.10 cm with morphology: Diagnosis_Tubular.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8Vm_g2ekfqBi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}